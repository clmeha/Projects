{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from scipy.optimize import fmin_cg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569L, 30L)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset with a binary Y\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "data.data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rlr(Lambda, X, Y):\n",
    "    '''Regularized Logistic Regression\n",
    "            Parameters: Lambda - postive float\n",
    "                        X      - matrix, data\n",
    "                        Y      - vector, binary\n",
    "            Returns:    w_hat  - vector, coefficient \n",
    "                                of the features in the \n",
    "                                decision function\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "    \n",
    "    def sigm(w, x):\n",
    "        '''Sigmoid function\n",
    "            Parameters: w      - vector, weights\n",
    "                        x      - vector, data\n",
    "            Returns:    1/(1+e^(w^Tx))\n",
    "        '''\n",
    "        return 1./(1 + np.exp(-x.dot(w)))\n",
    "\n",
    "    def F(w):\n",
    "        '''Negative Log Likelihood Function\n",
    "            Parameters: w      - vector, initial guess\n",
    "            Returns:    w_hat  - vector, coefficient of \n",
    "                                the features in the \n",
    "                                decision functioon\n",
    "        '''\n",
    "        return np.sum(Y *np.log(1+np.exp(-X.dot(w))) + (1-Y)* np.log(np.exp(X.dot(w))+1))+Lambda * np.transpose(w[1:]).dot(w[1:])\n",
    "\n",
    "    #Using Newton's Method to find w_hat\n",
    "    w_hat = fmin_cg(F, np.zeros(D), disp = False)\n",
    "    \n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_reg(Lambda, X, Y):\n",
    "    '''Logistic Regression\n",
    "\n",
    "        Parameters: Lambda - postive float\n",
    "                    X      - matrix, data\n",
    "                    Y      - vector, binary\n",
    "        Returns:    w_hat  - vector, coefficient \n",
    "                            of the features in the \n",
    "                            decision function\n",
    "    '''\n",
    "    N, D = X.shape\n",
    "\n",
    "    def sigm(w, x):\n",
    "        '''Sigmoid function\n",
    "            Parameters: w      - vector, weights\n",
    "                        x      - vector, data\n",
    "            Returns:    1/(1+e^(w^Tx))\n",
    "        '''\n",
    "        return 1./(1 + np.exp(-x.dot(w)))\n",
    "\n",
    "    def F(w):\n",
    "        '''Negative Log Likelihood Function\n",
    "            Parameters: w      - vector, initial guess\n",
    "            Returns:    w_hat  - vector, coefficient of \n",
    "                                the features in the \n",
    "                                decision functioon\n",
    "        '''\n",
    "        return np.sum(Y *np.log(1+np.exp(-X.dot(w))) + (1-Y)* np.log(np.exp(X.dot(w))+1))\n",
    "\n",
    "    #Using Newton's Method to find w_hat\n",
    "    w_hat = fmin_cg(F, np.zeros(D), disp = False)\n",
    "    \n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    '''Predict function for logistic regression (a.k.a. bias)\n",
    "        Parameters: x - vector, data\n",
    "                    w - vector, w_hat, or coefficients of the \n",
    "                        features of the decision function\n",
    "        Returns:    k - vector of true and false values\n",
    "    '''\n",
    "    def sigm(w, x):\n",
    "        '''Sigmoid function\n",
    "            Parameters: w      - vector, weights\n",
    "                        x      - vector, data\n",
    "            Returns:    1/(1+e^(w^Tx))\n",
    "        '''\n",
    "        return 1./(1 + np.exp(-x.dot(w)))\n",
    "    k = sigm(w, x)\n",
    "    return k >= .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:14: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time it take to fit and predict bias for Logistic Regression Solver is \t\t\t0.509999990463\n",
      "Time it take to fit and predict bias for Scikit learn Logisitic Regression Solver is \t0.0360000133514\n"
     ]
    }
   ],
   "source": [
    "#Timing Implemented Logistic Regression\n",
    "begin1 = time.time()\n",
    "predict(X, logistic_reg(Lambda, X, Y))\n",
    "end1 = time.time()\n",
    "diff1 = end1 - begin1\n",
    "print \"Time it take to fit and predict bias for Logistic Regression Solver is \\t\\t\\t\" + str(diff1)\n",
    "\n",
    "#Timing Sklearn Logistic Regression\n",
    "begin = time.time()\n",
    "model = LogisticRegression(solver = 'lbfgs', C = 1e15, tol=1e-6)\n",
    "coef = model.fit(X, Y)\n",
    "what = coef.coef_\n",
    "bias = coef.intercept_\n",
    "end = time.time()\n",
    "diff = end- begin\n",
    "print \"Time it take to fit and predict bias for Scikit learn Logisitic Regression Solver is \\t\" + str(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda is 10e-15 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-14 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-13 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-12 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-11 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-10 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-9 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-8 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-7 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-6 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-5 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-4 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-3 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-2 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e-1 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e0 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e1 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e2 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e3 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e4 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e5 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e6 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e7 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e8 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e9 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e10 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e11 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e12 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e13 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e14 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "When lambda is 10e15 then w_hat is [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:29: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:29: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time it takes to fit for Regularized Logistic Regression is 18.2969999313\n",
      "When there is no regularization term, then w_hat is  [  5.92174763e+00   1.69506705e+01   3.18917434e+01  -3.94051434e+02\n",
      "   1.04580611e-01  -2.04454136e-02  -1.64729501e-01  -8.84521047e-02\n",
      "   1.98774344e-01   8.54893915e-02  -2.58724903e-01   1.67105920e+00\n",
      "  -1.89139222e+00  -7.34051296e+01   1.05659749e-02   7.56194277e-03\n",
      "   3.86702884e-03   3.04734582e-03   2.80897806e-02   4.07896670e-03\n",
      "   2.76398397e+00   2.03503617e+01   1.01833415e+01  -9.52426276e+02\n",
      "   1.29822802e-01  -1.33046680e-01  -3.37845615e-01  -1.12588799e-01\n",
      "   2.60541015e-01   8.36311774e-02]\n",
      "Time it takes to fit for Logistic Regresstion is 0.542000055313\n",
      "Time it take to fit and predict bias for Scikit learn Logisitic Regression Solver is \t0.802999973297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clmeha\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#Timing Implemented Regularized Logistic Regression\n",
    "begin1 = time.time()\n",
    "for Lambda in xrange(-15, 16):\n",
    "    w_hat1 = rlr(Lambda, X, Y)\n",
    "    print \"When lambda is 10e\" + str(Lambda) + \" then w_hat is \" + str(w_hat1)\n",
    "end1 = time.time()\n",
    "diff1 = end1 - begin1\n",
    "print \"Time it takes to fit for Regularized Logistic Regression is \" + str(diff1)\n",
    "\n",
    "#Timing Implemented Logistic Regression\n",
    "begin2 = time.time()\n",
    "w_hat2 = logistic_reg(0, X, Y)\n",
    "print \"When there is no regularization term, then w_hat is  \" + str(w_hat2)\n",
    "end2 = time.time()\n",
    "diff2 = end2- begin2\n",
    "print \"Time it takes to fit for Logistic Regresstion is \" + str(diff2)\n",
    "\n",
    "#Timing Sklearn Regularized Logistic Regression\n",
    "begin = time.time()\n",
    "for Lambda in xrange(-15, 16):\n",
    "    lam = 2**Lambda\n",
    "    model = LogisticRegression(solver = 'lbfgs', C = 1./(2*lam), tol=1e-6)\n",
    "    coef = model.fit(X, Y)\n",
    "    what = coef.coef_\n",
    "end = time.time()\n",
    "diff = end- begin\n",
    "print \"Time it take to fit and predict bias for Scikit learn Logisitic Regression Solver is \\t\" + str(diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading data for Multiclass Logistic Regression\n",
    "data = datasets.load_digits()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "#Using Sklearn Multiclass Logistic Regression\n",
    "for Lambda in xrange(-15, 16):\n",
    "    lam = 2**Lambda\n",
    "    model = LogisticRegression(solver='lbfgs', multi_class='multinomial', C = 1./(2*lam), tol=1e-6)\n",
    "    coef = model.fit(X, Y)\n",
    "    w_hat0 = coef.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
